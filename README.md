# Async PostgreSQL Worker with FastAPI

A production-ready asynchronous job queue system built with FastAPI, PostgreSQL, and asyncio. This project demonstrates modern async Python patterns for building scalable background job processing systems.

## ğŸš€ Features

- **Async-First Design**: Built with asyncio and async/await patterns throughout
- **Job Queue System**: PostgreSQL-backed job queue with atomic job claiming
- **REST API**: FastAPI endpoints for job submission and status tracking
- **Background Worker**: Continuous job processing with graceful shutdown
- **Connection Pooling**: Efficient database connection management
- **Duplicate Detection**: Smart handling of duplicate job submissions
- **Concurrent Processing**: Multiple jobs processed simultaneously

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   FastAPI App   â”‚    â”‚   PostgreSQL     â”‚    â”‚ Background      â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚ Worker          â”‚
â”‚ â€¢ Job Creation  â”‚â—„â”€â”€â–ºâ”‚ â€¢ Job Queue      â”‚â—„â”€â”€â–ºâ”‚                 â”‚
â”‚ â€¢ Status Check  â”‚    â”‚ â€¢ Connection     â”‚    â”‚ â€¢ Job Processingâ”‚
â”‚ â€¢ Health Check  â”‚    â”‚   Pool           â”‚    â”‚ â€¢ Error Handlingâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- Python 3.8+
- PostgreSQL 12+
- Docker (optional, for easy PostgreSQL setup)

## ğŸ”§ Installation

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd async-worker-project
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Set up PostgreSQL**:
   
   **Option A: Using Docker**:
   ```bash
   docker-compose up -d
   ```
   
   **Option B: Local PostgreSQL**:
   ```bash
   createdb async_worker_db
   ```

4. **Run database migrations**:
   ```bash
   psql -d async_worker_db -f migrations/init.sql
   ```

5. **Configure environment**:
   ```bash
   export DATABASE_URL="postgresql://user:password@localhost/async_worker_db"
   ```

## ğŸš€ Quick Start

1. **Start the application**:
   ```bash
   uvicorn app.main:app --reload
   ```

2. **Create a job**:
   ```bash
   curl -X POST "http://localhost:8000/jobs" \
        -H "Content-Type: application/json" \
        -d '{"payload": {"task": "hello_world", "data": "test message"}}'
   ```

3. **Check job status**:
   ```bash
   curl "http://localhost:8000/jobs/{job_id}"
   ```

4. **Health check**:
   ```bash
   curl "http://localhost:8000/health"
   ```

## ğŸ“ Project Structure

```
async_worker/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # FastAPI application and lifespan management
â”‚   â”œâ”€â”€ models.py            # Pydantic models for request/response
â”‚   â”œâ”€â”€ database.py          # Database connection and operations
â”‚   â”œâ”€â”€ worker.py            # Background job processor
â”œâ”€â”€ migrations/
â”‚   â””â”€â”€ init.sql             # Database schema
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ test_api.py          # API endpoint tests
â”‚   â”œâ”€â”€ test_worker.py       # Worker logic tests
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ docker-compose.yml       # PostgreSQL for development
â””â”€â”€ README.md
```

## ğŸ”„ Job Lifecycle

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ queued  â”‚â”€â”€â”€â–ºâ”‚ processing  â”‚â”€â”€â”€â–ºâ”‚ completed â”‚    â”‚   failed   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚                                  â–²
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              (on error)
```

## ğŸ¯ API Endpoints

### Create Job
- **POST** `/jobs`
- **Body**: `{"payload": {"key": "value"}}`
- **Response**: `{"job_id": "uuid", "status": "queued"}`

### Get Job Status
- **GET** `/jobs/{job_id}`
- **Response**: 
  ```json
  {
    "job_id": "uuid",
    "status": "completed",
    "attempt_count": 1,
    "payload": {"key": "value"}
  }
  ```

### Health Check
- **GET** `/health`
- **Response**: `{"status": "healthy"}`

## ğŸ§ª Testing

Run the test suite:

```bash
# Install test dependencies
uv pip install -r requirements.txt

# Run tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=app --cov-report=html
```

## ğŸ—ï¸ Key Design Patterns

### 1. Async Database Operations
- Connection pooling for concurrent request handling
- Atomic job claiming with `FOR UPDATE SKIP LOCKED`
- Proper resource cleanup with async context managers

### 2. Job Queue Management
- PostgreSQL-based persistence for reliability
- Duplicate job detection and replacement
- Worker polling with graceful shutdown

### 3. Error Handling
- Structured error responses
- Basic retry logic (extensible for advanced patterns)
- Connection failure recovery

## ğŸ“ˆ Performance Characteristics

- **Concurrent Jobs**: Handles multiple jobs simultaneously
- **Database Connections**: Efficient pooling (2-5 connections by default)
- **Response Time**: Sub-100ms for job submission
- **Throughput**: Depends on job complexity and database performance

## ğŸ›£ï¸ Roadmap

### Phase 3: Robust Error Handling
- [ ] Exponential backoff with jitter
- [ ] Circuit breaker pattern
- [ ] Dead letter queue for failed jobs
- [ ] Comprehensive timeout hierarchy

### Phase 4: Real Task Execution
- [ ] Task registry system
- [ ] Email sending tasks
- [ ] File processing tasks
- [ ] Rate limiting and resource management

### Phase 5: Production Features
- [ ] Structured logging and monitoring
- [ ] Health check improvements
- [ ] Horizontal scaling support
- [ ] Admin dashboard

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ Development Notes

### Core Async Principles Applied

1. **Control Flow Management**: Just-in-time connection acquisition, proper use of `await` vs `asyncio.gather()`
2. **Critical Section Protection**: Atomic job claiming prevents race conditions
3. **Task Lifecycle Management**: Proper cleanup with async context managers
4. **Resource Management**: Connection pooling with health monitoring
5. **Graceful Degradation**: Error handling with structured responses

### Database Design Decisions

- **Connection as Parameter Pattern**: Enables flexible transaction boundaries
- **Atomic Operations**: Single SQL queries for consistency
- **Pool-Level Health Checks**: More efficient than per-connection validation
- **Structured Return Values**: Clear distinction between business logic and system errors

## ğŸ“š Learning Resources

This project demonstrates several advanced async Python concepts:

- [AsyncIO Documentation](https://docs.python.org/3/library/asyncio.html)
- [FastAPI Async Guide](https://fastapi.tiangolo.com/async/)
- [asyncpg Documentation](https://magicstack.github.io/asyncpg/current/)
- [PostgreSQL Connection Pooling](https://www.postgresql.org/docs/current/runtime-config-connection.html)

## ğŸ“„ License

This project is licensed under the MIT License - see the LICENSE file for details.

## ğŸ†˜ Troubleshooting

### Common Issues

**Database Connection Errors**:
```bash
# Check PostgreSQL is running
pg_isready -h localhost -p 5432

# Verify database exists
psql -l | grep async_worker_db
```

**Import Errors**:
```bash
# Ensure you're in the project root and PYTHONPATH is set
export PYTHONPATH=$(pwd)
```

**Worker Not Processing Jobs**:
- Check database connectivity
- Verify worker is running (check logs)
- Ensure jobs are in 'queued' status

### Debug Mode

Run with debug logging:
```bash
uvicorn app.main:app --reload --log-level debug
```

## ğŸ“Š Monitoring

Basic monitoring endpoints:

- `/health` - Application health status  
- `/metrics` - Basic application metrics (planned)
- Database queries for job queue depth and processing rates

For production deployments, consider integrating with:
- Prometheus for metrics collection
- Grafana for dashboards  
- Sentry for error tracking
- Structured logging with ELK stack